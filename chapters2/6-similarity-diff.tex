\section{Similarities and Differences}

The three papers collectively address \emph{Kubernetes optimizations} across different layers, targeting performance, scalability, and suitability for \emph{edge and IoT environments}. All emphasize \emph{empirical evaluation}: \cite{C1} and \cite{C3} benchmark existing components (CNI plugins and lightweight CODs, respectively), while \cite{C2} combines performance analysis of etcd with a proposal for architectural redesign. A shared theme is the critical examination of Kubernetes (sub)systems—networking (\cite{C1}), control-plane storage (\cite{C2}), and orchestration frameworks (\cite{C3})—to address inefficiencies in distributed systems.

\cite{C1} and \cite{C3} focus on aiding practitioners in \emph{selecting optimal solutions}. Both employ granular performance metrics, such as CPU cycles per packet (\cite{C1}) and control-plane latency (\cite{C3}). In contrast, \cite{C2} identifies etcd’s strong consistency as a scalability bottleneck for edge deployments and proposes replacing it with an eventually consistent CRDT-based store. This positions \cite{C2} as a \emph{forward-looking architectural critique}, whereas \cite{C1} and \cite{C3} are grounded in comparative analysis of existing tools.

\cite{C2} and \cite{C3} differ from \cite{C1} in the specific computing environment targeted. They both specifically focus on edge computing, while \cite{C1} provides a more general scope. However, while \cite{C1} is not explicitly edge-focused, it provides insights relevant to edge networking, such as the impact of CNI choices on pod communication latency. Methodologically, \cite{C1} and \cite{C2} analyze low-level interactions: \cite{C1} dissects kernel-space overheads (e.g., Netfilter, eBPF), while \cite{C2} benchmarks etcd’s write latency and throughput. \cite{C3} adopts a higher-level approach, integrating system-wide metrics (e.g., CPU, disk usage) and application-layer performance (CoAP workloads).